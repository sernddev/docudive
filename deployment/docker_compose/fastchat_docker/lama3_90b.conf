[supervisord]
nodaemon=true

[program:lama3_90bworker]
command=python3 -m fastchat.serve.vllm_worker --model-name 'Llama-3.2-90B-Vision-Instruct' --model-path /app/models/Llama/Llama-3.2-90B-Vision-Instruct --host '0.0.0.0' --num-gpus 4 --dtype bfloat16
stdout_logfile=/dev/stdout 
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr 
stderr_logfile_maxbytes=0
autorestart=true 
startretries=100 
startsecs=60