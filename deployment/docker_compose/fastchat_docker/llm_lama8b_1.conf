[supervisord]
nodaemon=true

[program:lama8_worker]
command=python3 -m fastchat.serve.vllm_worker --model-name 'Meta-Llama-3.1-8B-Instruct' --model-path /app/models/Llama/Meta-Llama-3.1-8B-Instruct/snapshots/5206a32e0bd3067aef1ce90f5528ade7d866253f --host '0.0.0.0' --num-gpus 4 --dtype bfloat16
stdout_logfile=/dev/stdout 
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr 
stderr_logfile_maxbytes=0
autorestart=true 
startretries=100 
startsecs=60