[supervisord]
nodaemon=true

[program:controller]
command=python3 -m fastchat.serve.controller --host '0.0.0.0'
stdout_logfile=/dev/stdout 
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr 
stderr_logfile_maxbytes=0
autorestart=true 
startretries=100 
startsecs=60

[program:vllm_worker]
command=python3 -m fastchat.serve.vllm_worker --model-name 'Meta-Llama-3-70B-Instruct' --model-path /app/models/Llama/Meta-Llama-3-70B-Instruct --host '0.0.0.0' --num-gpus 4 --dtype bfloat16
stdout_logfile=/dev/stdout 
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr 
stderr_logfile_maxbytes=0
autorestart=true 
startretries=100 
startsecs=60

[program:openai_api_server]
command=python3 -m fastchat.serve.openai_api_server --host '0.0.0.0' --port 9198
stdout_logfile=/dev/stdout 
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr 
stderr_logfile_maxbytes=0
autorestart=true 
startretries=100 
startsecs=60
